{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+by/AhByXmLR3tPS8XMNh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandhyajnv/Blind-Image-Quality-Assessment/blob/main/DBCNN_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SCNN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkcHeFzIGlBa",
        "outputId": "494de4e0-80ed-4552-c44f-0711a156f325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SCNN\n",
            "  Downloading scnn-1.3.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from SCNN) (1.22.4)\n",
            "Collecting theano (from SCNN)\n",
            "  Downloading Theano-1.0.5.tar.gz (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lasagne>=0.1dev (from SCNN)\n",
            "  Downloading Lasagne-0.1.tar.gz (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.1/125.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from SCNN) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SCNN) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SCNN) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SCNN) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SCNN) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SCNN) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SCNN) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SCNN) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->SCNN) (2.8.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from theano->SCNN) (1.10.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from theano->SCNN) (1.16.0)\n",
            "Building wheels for collected packages: SCNN, lasagne, theano\n",
            "  Building wheel for SCNN (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SCNN: filename=scnn-1.3.1-py3-none-any.whl size=28187 sha256=f0fa470416098da867f1bf1cca82c374dca6aaf1deeddccec03fac7f037d1b78\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/ca/36/d9e10eb16524ad9fbebdee0a4f8cba4fdf89ebb019ad35c9c8\n",
            "  Building wheel for lasagne (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lasagne: filename=Lasagne-0.1-py3-none-any.whl size=79267 sha256=5841dd67f63899a87c9531e98ae1ee1fb09860cb3c6b3071e6aab124e04d6b60\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/22/9f/1512e23c2556397304b730376ae4a7b34c5a6e8d68c1273917\n",
            "  Building wheel for theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for theano: filename=Theano-1.0.5-py3-none-any.whl size=2668109 sha256=13f6e7deeb649fcf48b10910c76f7a39323375e94888501465e4ebfb2b68d0ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/e6/7d/2267d21a99e4ab8276f976f293b4ff23f50c9d809f4a216ebb\n",
            "Successfully built SCNN lasagne theano\n",
            "Installing collected packages: lasagne, theano, SCNN\n",
            "Successfully installed SCNN-1.3.1 lasagne-0.1 theano-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "Sx34nbOxG1wZ",
        "outputId": "b2513ece-8ebd-419d-9d64-a80eb600390d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-830fcd4089ec>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scnn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__author__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jatwood'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgraph_scnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphSCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'SCNN' from partially initialized module 'scnn' (most likely due to a circular import) (/usr/local/lib/python3.10/dist-packages/scnn/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC1eIodeLqQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "327b50d6-c1f3-4b05-b13f-c94c1fc88689"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d5cb34ef0156>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mSCNN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'SCNN'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from SCNN import SCNN\n",
        "from PIL import Image\n",
        "from scipy import stats\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "\n",
        "def pil_loader(path):\n",
        "\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "\n",
        "def accimage_loader(path):\n",
        "    import accimage\n",
        "    try:\n",
        "        return accimage.Image(path)\n",
        "    except IOError:\n",
        "        # Potentially a decoding problem, fall back to PIL.Image\n",
        "        return pil_loader(path)\n",
        "\n",
        "\n",
        "def default_loader(path):\n",
        "    from torchvision import get_image_backend\n",
        "    if get_image_backend() == 'accimage':\n",
        "        return accimage_loader(path)\n",
        "    else:\n",
        "        return pil_loader(path)\n",
        "\n",
        "\n",
        "\n",
        "class DBCNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, scnn_root, options):\n",
        "        \"\"\"Declare all needed layers.\"\"\"\n",
        "        nn.Module.__init__(self)\n",
        "        # Convolution and pooling layers of VGG-16.\n",
        "        self.features1 = torchvision.models.vgg16(pretrained=True).features\n",
        "        self.features1 = nn.Sequential(*list(self.features1.children())\n",
        "                                            [:-1])\n",
        "        scnn = SCNN()\n",
        "        scnn = torch.nn.DataParallel(scnn).cuda()\n",
        "\n",
        "        scnn.load_state_dict(torch.load(scnn_root))\n",
        "        self.features2 = scnn.module.features\n",
        "\n",
        "        # Linear classifier.\n",
        "        self.fc = torch.nn.Linear(512*128, 1)\n",
        "\n",
        "        if options['fc'] == True:\n",
        "            # Freeze all previous layers.\n",
        "            for param in self.features1.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.features2.parameters():\n",
        "                param.requires_grad = False\n",
        "            # Initialize the fc layers.\n",
        "            nn.init.kaiming_normal_(self.fc.weight.data)\n",
        "            if self.fc.bias is not None:\n",
        "                nn.init.constant_(self.fc.bias.data, val=0)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass of the network.\n",
        "        \"\"\"\n",
        "        N = X.size()[0]\n",
        "        X1 = self.features1(X)\n",
        "        H = X1.size()[2]\n",
        "        W = X1.size()[3]\n",
        "        assert X1.size()[1] == 512\n",
        "        X2 = self.features2(X)\n",
        "        H2 = X2.size()[2]\n",
        "        W2 = X2.size()[3]\n",
        "        assert X2.size()[1] == 128\n",
        "\n",
        "        if (H != H2) | (W != W2):\n",
        "            X2 = F.upsample_bilinear(X2,(H,W))\n",
        "\n",
        "        X1 = X1.view(N, 512, H*W)\n",
        "        X2 = X2.view(N, 128, H*W)\n",
        "        X = torch.bmm(X1, torch.transpose(X2, 1, 2)) / (H*W)  # Bilinear\n",
        "        assert X.size() == (N, 512, 128)\n",
        "        X = X.view(N, 512*128)\n",
        "        X = torch.sqrt(X + 1e-8)\n",
        "        X = torch.nn.functional.normalize(X)\n",
        "        X = self.fc(X)\n",
        "        assert X.size() == (N, 1)\n",
        "        return X\n",
        "\n",
        "\n",
        "class DBCNNManager(object):\n",
        "    def __init__(self, options, path):\n",
        "\n",
        "        self._options = options\n",
        "        self._path = path\n",
        "\n",
        "        # Network.\n",
        "        self._net = torch.nn.DataParallel(DBCNN(self._path['scnn_root'], self._options), device_ids=[0]).cuda()\n",
        "        if self._options['fc'] == False:\n",
        "            self._net.load_state_dict(torch.load(path['fc_root']))\n",
        "\n",
        "        print(self._net)\n",
        "        # Criterion.\n",
        "        self._criterion = torch.nn.MSELoss().cuda()\n",
        "\n",
        "        # Solver.\n",
        "        if self._options['fc'] == True:\n",
        "            self._solver = torch.optim.SGD(\n",
        "                    self._net.module.fc.parameters(), lr=self._options['base_lr'],\n",
        "                    momentum=0.9, weight_decay=self._options['weight_decay'])\n",
        "        else:\n",
        "            self._solver = torch.optim.Adam(\n",
        "                    self._net.module.parameters(), lr=self._options['base_lr'],\n",
        "                    weight_decay=self._options['weight_decay'])\n",
        "\n",
        "\n",
        "        if (self._options['dataset'] == 'live') | (self._options['dataset'] == 'livec'):\n",
        "            if self._options['dataset'] == 'live':\n",
        "                crop_size = 432\n",
        "            else:\n",
        "                crop_size = 448\n",
        "            train_transforms = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.RandomHorizontalFlip(),\n",
        "            torchvision.transforms.RandomCrop(size=crop_size),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                             std=(0.229, 0.224, 0.225))\n",
        "            ])\n",
        "        elif (self._options['dataset'] == 'csiq') | (self._options['dataset'] == 'tid2013'):\n",
        "            train_transforms = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.RandomHorizontalFlip(),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                             std=(0.229, 0.224, 0.225))\n",
        "            ])\n",
        "        elif self._options['dataset'] == 'mlive':\n",
        "            train_transforms = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize((570,960)),\n",
        "            torchvision.transforms.RandomHorizontalFlip(),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                             std=(0.229, 0.224, 0.225))\n",
        "            ])\n",
        "\n",
        "\n",
        "        test_transforms = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                             std=(0.229, 0.224, 0.225))\n",
        "        ])\n",
        "\n",
        "\n",
        "        if self._options['dataset'] == 'live':\n",
        "            import LIVEFolder\n",
        "            train_data = LIVEFolder.LIVEFolder(\n",
        "                    root=self._path['live'], loader = default_loader, index = self._options['train_index'],\n",
        "                    transform=train_transforms)\n",
        "            test_data = LIVEFolder.LIVEFolder(\n",
        "                    root=self._path['live'], loader = default_loader, index = self._options['test_index'],\n",
        "                    transform=test_transforms)\n",
        "        elif self._options['dataset'] == 'livec':\n",
        "            import LIVEChallengeFolder\n",
        "            train_data = LIVEChallengeFolder.LIVEChallengeFolder(\n",
        "                    root=self._path['livec'], loader = default_loader, index = self._options['train_index'],\n",
        "                    transform=train_transforms)\n",
        "            test_data = LIVEChallengeFolder.LIVEChallengeFolder(\n",
        "                    root=self._path['livec'], loader = default_loader, index = self._options['test_index'],\n",
        "                    transform=test_transforms)\n",
        "        else:\n",
        "            raise AttributeError('Only support LIVE and LIVEC right now!')\n",
        "        self._train_loader = torch.utils.data.DataLoader(\n",
        "            train_data, batch_size=self._options['batch_size'],\n",
        "            shuffle=True, num_workers=0, pin_memory=True)\n",
        "        self._test_loader = torch.utils.data.DataLoader(\n",
        "            test_data, batch_size=1,\n",
        "            shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the network.\"\"\"\n",
        "        print('Training.')\n",
        "        best_srcc = 0.0\n",
        "        best_epoch = None\n",
        "        print('Epoch\\tTrain loss\\tTrain_SRCC\\tTest_SRCC\\tTest_PLCC')\n",
        "        for t in range(self._options['epochs']):\n",
        "            epoch_loss = []\n",
        "            pscores = []\n",
        "            tscores = []\n",
        "            num_total = 0\n",
        "            for X, y in self._train_loader:\n",
        "                # Data.\n",
        "                X = torch.tensor(X.cuda())\n",
        "                y = torch.tensor(y.cuda())\n",
        "\n",
        "\n",
        "                # Clear the existing gradients.\n",
        "                self._solver.zero_grad()\n",
        "                # Forward pass.\n",
        "                score = self._net(X)\n",
        "                loss = self._criterion(score, y.view(len(score),1).detach())\n",
        "                epoch_loss.append(loss.item())\n",
        "                # Prediction.\n",
        "                num_total += y.size(0)\n",
        "                pscores = pscores +  score.cpu().tolist()\n",
        "                tscores = tscores + y.cpu().tolist()\n",
        "                # Backward pass.\n",
        "                loss.backward()\n",
        "                self._solver.step()\n",
        "            train_srcc, _ = stats.spearmanr(pscores,tscores)\n",
        "            test_srcc, test_plcc = self._consitency(self._test_loader)\n",
        "            if test_srcc > best_srcc:\n",
        "                best_srcc = test_srcc\n",
        "                best_epoch = t + 1\n",
        "                print('*', end='')\n",
        "                pwd = os.getcwd()\n",
        "                if self._options['fc'] == True:\n",
        "                    modelpath = os.path.join(pwd,'fc_models',('net_params' + '_best' + '.pkl'))\n",
        "                else:\n",
        "                    modelpath = os.path.join(pwd,'db_models',('net_params' + '_best' + '.pkl'))\n",
        "                torch.save(self._net.state_dict(), modelpath)\n",
        "\n",
        "            print('%d\\t%4.3f\\t\\t%4.4f\\t\\t%4.4f\\t%4.4f' %\n",
        "                  (t+1, sum(epoch_loss) / len(epoch_loss), train_srcc, test_srcc, test_plcc))\n",
        "\n",
        "        print('Best at epoch %d, test srcc %f' % (best_epoch, best_srcc))\n",
        "        return best_srcc\n",
        "\n",
        "    def _consitency(self, data_loader):\n",
        "        self._net.train(False)\n",
        "        num_total = 0\n",
        "        pscores = []\n",
        "        tscores = []\n",
        "        for X, y in data_loader:\n",
        "            # Data.\n",
        "            X = torch.tensor(X.cuda())\n",
        "            y = torch.tensor(y.cuda())\n",
        "\n",
        "\n",
        "            # Prediction.\n",
        "            score = self._net(X)\n",
        "            pscores = pscores +  score[0].cpu().tolist()\n",
        "            tscores = tscores + y.cpu().tolist()\n",
        "\n",
        "            num_total += y.size(0)\n",
        "        test_srcc, _ = stats.spearmanr(pscores,tscores)\n",
        "        test_plcc, _ = stats.pearsonr(pscores,tscores)\n",
        "        self._net.train(True)  # Set the model to training phase\n",
        "        return test_srcc, test_plcc\n",
        "\n",
        "def main():\n",
        "    \"\"\"The main function.\"\"\"\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Train DB-CNN for BIQA.')\n",
        "    parser.add_argument('--base_lr', dest='base_lr', type=float, default=1e-5,\n",
        "                        help='Base learning rate for training.')\n",
        "    parser.add_argument('--batch_size', dest='batch_size', type=int,\n",
        "                        default=8, help='Batch size.')\n",
        "    parser.add_argument('--epochs', dest='epochs', type=int,\n",
        "                        default=50, help='Epochs for training.')\n",
        "    parser.add_argument('--weight_decay', dest='weight_decay', type=float,\n",
        "                        default=5e-4, help='Weight decay.')\n",
        "    parser.add_argument('--dataset',dest='dataset',type=str,default='live',\n",
        "                        help='dataset: live|csiq|tid2013|livec|mlive')\n",
        "\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    if args.base_lr <= 0:\n",
        "        raise AttributeError('--base_lr parameter must >0.')\n",
        "    if args.batch_size <= 0:\n",
        "        raise AttributeError('--batch_size parameter must >0.')\n",
        "    if args.epochs < 0:\n",
        "        raise AttributeError('--epochs parameter must >=0.')\n",
        "    if args.weight_decay <= 0:\n",
        "        raise AttributeError('--weight_decay parameter must >0.')\n",
        "\n",
        "    options = {\n",
        "        'base_lr': args.base_lr,\n",
        "        'batch_size': args.batch_size,\n",
        "        'epochs': args.epochs,\n",
        "        'weight_decay': args.weight_decay,\n",
        "        'dataset':args.dataset,\n",
        "        'fc': [],\n",
        "        'train_index': [],\n",
        "        'test_index': []\n",
        "    }\n",
        "\n",
        "    path = {\n",
        "        'live': os.path.join('dataset','databaserelease2'),\n",
        "        'csiq': os.path.join('dataset','CSIQ'),\n",
        "        'tid2013': os.path.join('dataset','TID2013'),\n",
        "        'livec': os.path.join('dataset','ChallengeDB_release'),\n",
        "        'mlive': os.path.join('dataset','LIVEmultidistortiondatabase'),\n",
        "        'fc_model': os.path.join('fc_models'),\n",
        "        'scnn_root': os.path.join('pretrained_scnn','scnn.pkl'),\n",
        "        'fc_root': os.path.join('fc_models','net_params_best.pkl'),\n",
        "        'db_model': os.path.join('db_models')\n",
        "    }\n",
        "\n",
        "\n",
        "    if options['dataset'] == 'live':\n",
        "        index = list(range(0,29))\n",
        "    elif options['dataset'] == 'csiq':\n",
        "        index = list(range(0,30))\n",
        "    elif options['dataset'] == 'tid2013':\n",
        "        index = list(range(0,25))\n",
        "    elif options['dataset'] == 'mlive':\n",
        "        index = list(range(0,15))\n",
        "    elif options['dataset'] == 'livec':\n",
        "        index = list(range(0,1162))\n",
        "\n",
        "\n",
        "    lr_backup = options['base_lr']\n",
        "    srcc_all = np.zeros((1,10),dtype=np.float)\n",
        "\n",
        "    for i in range(0,10):\n",
        "        #randomly split train-test set\n",
        "        random.shuffle(index)\n",
        "        train_index = index[0:round(0.8*len(index))]\n",
        "        test_index = index[round(0.8*len(index)):len(index)]\n",
        "\n",
        "        options['train_index'] = train_index\n",
        "        options['test_index'] = test_index\n",
        "        #train the fully connected layer only\n",
        "        options['fc'] = True\n",
        "        options['base_lr'] = 1e-3\n",
        "        manager = DBCNNManager(options, path)\n",
        "        best_srcc = manager.train()\n",
        "\n",
        "        #fine-tune all model\n",
        "        options['fc'] = False\n",
        "        options['base_lr'] = lr_backup\n",
        "        manager = DBCNNManager(options, path)\n",
        "        best_srcc = manager.train()\n",
        "\n",
        "        srcc_all[0][i] = best_srcc\n",
        "\n",
        "    srcc_mean = np.mean(srcc_all)\n",
        "    print(srcc_all)\n",
        "    print('average srcc:%4.4f' % (srcc_mean))\n",
        "    return best_srcc\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}